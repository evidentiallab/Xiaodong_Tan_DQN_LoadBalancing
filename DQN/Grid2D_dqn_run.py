from Grid2D_dqn_train import Deeplearningfrom ILP.Graph_generator import GraphGeneratorGrid2Dimport matplotlib.pyplot as pltimport numpy as nptest_one = 0Generate = 1Train = 0Test = 0Graph_m = 5Graph_n = Graph_mpercentage = 1Network_pth = 5000Train_episodes = 50001Test_total = 500Average_rate = 10plot_err_list,plot_right_list = [100.],[0.]train_episodes = 0wrong_err,right_rate = 0,0graph_path = 'Graph_and_Net_Model/model/22120727_55/graph_model.pkl'Graph_model = GraphGeneratorGrid2D(Graph_m, Graph_n, percentage).load_graph_model(graph_path)DQN_model = Deeplearning(Graph_model)directory  = Graph_model.directorydef find_error_path(p):  # local optimal or not    return 1 if len(p) >= 29 else 0def DQN_right_ILP(p1, p2):  # equal to the ILP Path    return 1 if p1 == p2 else 0def plot_rate(lis, name):    plt.plot(lis)    plt.title(str(name))    plt.xlabel('Train Step(k)')    plt.ylabel('%')    plt.ylim(0,100)    plt.savefig(directory + name + '.png')    plt.close()def test_and_plot():    DQN_path, ILP_path, reward = DQN_model.take_action(plot=False, net_path=net_path)    print(ILP_path, '\n', DQN_path, '\n', reward, '\n')if Generate:    """generate model"""    Graph_model = GraphGeneratorGrid2D(Graph_m, Graph_n, percentage)    Graph_model.save_graph_model()  # save the graph model    # print the ILP result simple and balancing    Graph_model.draw_and_save("generator.png")if Train:    """Load graph model and Training"""    DQN_model.train(Network_pth,Train_episodes)if test_one:    net_path = "Graph_and_Net_Model/model/22120727_55/percent_1/policy_net-50000.pth"    DQN_path, ILP_path, reward,t = DQN_model.take_action(plot=True, net_path=net_path)    print(DQN_path, '\n',DQN_path,'\n', reward,'\n',t)if Test:    """Load graph model Test the model"""    for train_episodes in range(Network_pth,Train_episodes,Network_pth):        net_path = 'Graph_and_Net_Model/model/22082643_55/policy_net-' + str(train_episodes) + '.pth'        right_list,error_list = [],[]        for j in range(Average_rate):            right,error = 0,0            for i in range(Test_total):                DQN_path ,ILP_path, reward = DQN_model.take_action(plot=False, net_path=net_path)                error += find_error_path(DQN_path)                right += DQN_right_ILP(DQN_path, ILP_path)            a = np.round((error / Test_total * 100), 2)            b = np.round((right / Test_total * 100), 2)            error_list.append(a)            right_list.append(b)            wrong_err = np.round(np.mean(error_list), 2)            right_rate = np.round(np.mean(right_list), 2)        plot_err_list.append(wrong_err)        plot_right_list.append(right_rate)        # print('error_list:',error_list,'\n','right_list_Heu:',right_list_Heu)        print('%4.2f' % wrong_err,'%','%4.2f' % right_rate,'%',train_episodes)    print(max(plot_right_list),'plot_right_list:',plot_right_list,'\n','plot_err_list:',plot_err_list)    plot_rate(plot_err_list,'error_rate')    plot_rate(plot_right_list, 'eright_rate')    # plot_err_list.to_csv(directory,encoding='utf-8')    # plot_right_list.to_csv(directory, encoding='utf-8')# policy = DQN_model.policy.load_net_pkl(net_path)# DQN_path ,reward = DQN_model.take_action(policy=policy,plot=True)# policy = DQN_model.policy.load_state_dict(torch.load(net_path,map_location=lambda storage, loc: storage))